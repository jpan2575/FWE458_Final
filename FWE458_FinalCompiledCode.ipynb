{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlOyVR9/bICmGMefz3StHK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpan2575/FWE458_Final/blob/main/FWE458_FinalCompiledCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Read the CSV file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "filedir = '/content/drive/MyDrive/FWE458/'"
      ],
      "metadata": {
        "id": "HuZ4OWDcXbln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dcS8PGeW3An"
      },
      "outputs": [],
      "source": [
        "#Compiling Datasets\n",
        "import pandas as pd\n",
        "\n",
        "# --- Configuration ---\n",
        "# Base file from the previous step\n",
        "base_file = \"fully_combined_dataset.csv\"\n",
        "\n",
        "new_annual_files_info = {\n",
        "    \"RealEstateDev\": \"realestaste development.csv\",\n",
        "    \"WasteDisposalVol\": \"wastedisposalvolume.csv\",\n",
        "    \"NumWasteFactories\": \"numberofwastefactorie.csv\",\n",
        "    \"MajorEnergyConsumption\": \"consumption of major energy.csv\",\n",
        "    \"IndustryValue_Overall\": \"AnnualbyProvince industryvalue.csv\", # To distinguish from previous 'IndustryValue'\n",
        "    \"IndustrialEnterprise\": \"AnnualbyProvinceindustrial enterprise.csv\"\n",
        "}\n",
        "header_row_annual = 3 # Standard for these types of files\n",
        "\n",
        "# --- Load the current base dataset ---\n",
        "try:\n",
        "    current_df = pd.read_csv(base_file)\n",
        "    print(f\"Successfully loaded base dataset: {base_file}\")\n",
        "    print(f\"Base dataset shape: {current_df.shape}\")\n",
        "    print(f\"Base dataset columns: {current_df.columns.tolist()}\")\n",
        "\n",
        "    # Ensure 'Region' and 'Year' in current_df are suitable for merging\n",
        "    if 'Region' not in current_df.columns:\n",
        "        raise ValueError(\"Critical Error: 'Region' column missing in the base_df.\")\n",
        "    if 'Year' not in current_df.columns:\n",
        "        raise ValueError(\"Critical Error: 'Year' column missing in the base_df.\")\n",
        "\n",
        "    current_df['Region'] = current_df['Region'].astype(str).str.strip()\n",
        "    current_df['Year'] = current_df['Year'].astype(int)\n",
        "\n",
        "    # --- Process and merge each new annual file ---\n",
        "    for value_col_name_prefix, file_name in new_annual_files_info.items():\n",
        "        print(f\"\\nProcessing new file: {file_name} (to be prefixed as {value_col_name_prefix})\")\n",
        "\n",
        "        # Check if a column with this prefix/name already exists to prevent accidental overwrite\n",
        "        # (though left merge would add suffixes like _x, _y, better to have unique names)\n",
        "        if value_col_name_prefix in current_df.columns:\n",
        "            print(f\"Warning: Column '{value_col_name_prefix}' already exists in the dataframe. Skipping merge for this file to avoid duplication or conflict. Please choose a different name if this data is new.\")\n",
        "            # Alternative: Add a suffix, or allow user to decide. For now, skip.\n",
        "            # current_df.rename(columns={value_col_name_prefix: value_col_name_prefix + \"_old\"}, inplace=True)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            annual_df_raw = pd.read_csv(file_name, header=header_row_annual)\n",
        "\n",
        "            annual_df_raw = annual_df_raw.dropna(axis=1, how='all') # Drop fully empty year columns\n",
        "\n",
        "            # Identify 'Region' column (usually the first one)\n",
        "            if annual_df_raw.empty or annual_df_raw.shape[1] == 0:\n",
        "                print(f\"Warning: {file_name} is empty or has no columns after dropping all-NaN columns. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            id_vars_annual = annual_df_raw.columns[0]\n",
        "            if id_vars_annual != 'Region':\n",
        "                 # If the first column is not named Region, rename it.\n",
        "                 print(f\"Info: First column in {file_name} is '{id_vars_annual}'. Renaming to 'Region'.\")\n",
        "                 annual_df_raw.rename(columns={id_vars_annual: 'Region'}, inplace=True)\n",
        "                 id_vars_annual = 'Region' # Update the variable\n",
        "\n",
        "            annual_df_melted = pd.melt(annual_df_raw, id_vars=['Region'],\n",
        "                                       var_name='Year', value_name=value_col_name_prefix)\n",
        "\n",
        "            # Clean 'Year'\n",
        "            annual_df_melted['Year'] = pd.to_numeric(annual_df_melted['Year'], errors='coerce')\n",
        "            annual_df_melted.dropna(subset=['Year'], inplace=True)\n",
        "            annual_df_melted['Year'] = annual_df_melted['Year'].astype(int)\n",
        "\n",
        "            # Clean 'Region'\n",
        "            annual_df_melted.dropna(subset=['Region'], inplace=True)\n",
        "            annual_df_melted['Region'] = annual_df_melted['Region'].astype(str).str.strip()\n",
        "            # Remove common metadata rows\n",
        "            metadata_rows = ['Data Sourcesï¼šNational Bureau of Statistics',\n",
        "                             'Data Source: National Bureau of Statistics',\n",
        "                             'Data Sources: National Bureau of Statistics',\n",
        "                             'Data Source: National Bureau of Statistics of China']\n",
        "            annual_df_melted = annual_df_melted[~annual_df_melted['Region'].isin(metadata_rows)]\n",
        "\n",
        "            # Clean value column (convert to numeric, coercing errors)\n",
        "            annual_df_melted[value_col_name_prefix] = pd.to_numeric(annual_df_melted[value_col_name_prefix], errors='coerce')\n",
        "\n",
        "\n",
        "            # Merge with the current combined dataframe\n",
        "            print(f\"Merging {value_col_name_prefix} into the main dataframe...\")\n",
        "            print(f\"Shapes before merge: current_df={current_df.shape}, annual_df_melted ({value_col_name_prefix})={annual_df_melted.shape}\")\n",
        "\n",
        "            # Store original columns to check if new ones were added or if merge resulted in _x, _y\n",
        "            original_cols = set(current_df.columns)\n",
        "            current_df = pd.merge(current_df, annual_df_melted, on=['Region', 'Year'], how='left')\n",
        "            new_cols = set(current_df.columns) - original_cols\n",
        "\n",
        "            print(f\"Shapes after merge: current_df={current_df.shape}\")\n",
        "            if not new_cols and value_col_name_prefix not in current_df.columns and f\"{value_col_name_prefix}_x\" in current_df.columns:\n",
        "                 print(f\"Warning: Merge might have resulted in suffixed columns (e.g., _x, _y) for {value_col_name_prefix} due to pre-existing column.\")\n",
        "            elif value_col_name_prefix in current_df.columns:\n",
        "                 print(f\"NaNs introduced in '{value_col_name_prefix}': {current_df[value_col_name_prefix].isna().sum()} / {len(current_df)}\")\n",
        "            else:\n",
        "                print(f\"Column {value_col_name_prefix} was not properly added. Columns added: {new_cols}\")\n",
        "\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File not found - {file_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing {file_name}: {e}\")\n",
        "\n",
        "    # --- Final inspection and save ---\n",
        "    print(\"\\nFinal combined dataframe head:\")\n",
        "    print(current_df.head())\n",
        "    print(\"\\nFinal combined dataframe info:\")\n",
        "    current_df.info()\n",
        "    print(f\"\\nFinal combined dataframe shape: {current_df.shape}\")\n",
        "\n",
        "    output_file_name = \"final_augmented_dataset.csv\"\n",
        "    current_df.to_csv(output_file_name, index=False)\n",
        "    print(f\"\\nSuccessfully saved the further augmented dataset to: {output_file_name}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Base file '{base_file}' not found. Please ensure it was generated from the previous step.\")\n",
        "except ValueError as ve:\n",
        "    print(ve)\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Time Series\n",
        "df_filtered = df[['time', 'avg']].dropna()\n",
        "df_filtered['time'] = df_filtered['time'].astype(int)\n",
        "df_filtered['avg'] = pd.to_numeric(df_filtered['avg'], errors='coerce')\n",
        "df_filtered = df_filtered.dropna()\n",
        "\n",
        "# Remove outliers using IQR\n",
        "Q1 = df_filtered['avg'].quantile(0.25)\n",
        "Q3 = df_filtered['avg'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "df_no_outliers = df_filtered[(df_filtered['avg'] >= lower_bound) & (df_filtered['avg'] <= upper_bound)]\n",
        "\n",
        "# Plot the time series\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df_no_outliers['time'], df_no_outliers['avg'], marker='o', linestyle='', alpha=0.7)\n",
        "plt.title('Time Series Plot Without Outliers')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Î¼g/kg')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l8KzmmKDXHy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Raw Time Series\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['time'], df['avg'], marker='o', linestyle='', alpha=0.7)\n",
        "plt.title('Raw Data Time Series Plot')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Î¼g/kg')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z2hcI3gCXpJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# df\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#Elbow\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wcss = []\n",
        "for i in range(1, 11):  # Try k values from 1 to 10\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
        "    kmeans.fit(X_scaled)  # Use scaled features\n",
        "    wcss.append(kmeans.inertia_)  # Inertia is the WCSS\n",
        "\n",
        "plt.plot(range(1, 11), wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-DaCzWM6XvFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# 3D plot of the clusters\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(data_no_outliers['avg'], data_no_outliers['min'], data_no_outliers['max'],\n",
        "                    c=data_no_outliers['cluster'], cmap='viridis', s=50)\n",
        "ax.set_xlabel('Average Concentration')\n",
        "ax.set_ylabel('Minimum Concentration')\n",
        "ax.set_zlabel('Maximum Concentration')\n",
        "plt.title('3D Visualization of Clusters (k=4, Outliers Removed)')\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# Scatterplot of 'avg' vs 'min', colored by cluster\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data_no_outliers['avg'], data_no_outliers['min'], c=data_no_outliers['cluster'], cmap='viridis', s=50)\n",
        "plt.xlabel('Average Concentration')\n",
        "plt.ylabel('Minimum Concentration')\n",
        "plt.title('Scatterplot of Average vs Minimum Concentration, Colored by Cluster (k=4, Outliers Removed)')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# Scatterplot of 'avg' vs 'max', colored by cluster\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data_no_outliers['avg'], data_no_outliers['max'], c=data_no_outliers['cluster'], cmap='viridis', s=50)\n",
        "plt.xlabel('Average Concentration')\n",
        "plt.ylabel('Maximum Concentration')\n",
        "plt.title('Scatterplot of Average vs Maximum Concentration, Colored by Cluster (k=4, Outliers Removed)')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YzsnEmJDX_St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score  # Import silhouette_score\n",
        "\n",
        "# Calculate Silhouette Score\n",
        "silhouette_avg = silhouette_score(data_scaled, clusters)\n",
        "print(f\"Silhouette Score: {silhouette_avg:.3f}\")"
      ],
      "metadata": {
        "id": "P6p8ZzN2YJ0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#China Map\n",
        "!pip install cartopy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "\n",
        "# ... (previous code for data loading and cleaning) ...\n",
        "\n",
        "# Convert 'avg' column to numeric, handling errors\n",
        "df['avg'] = pd.to_numeric(df['avg'], errors='coerce')\n",
        "\n",
        "# Create a figure and axes with cartopy projection\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
        "\n",
        "# Add basemap features for China\n",
        "ax.add_feature(cfeature.LAND)\n",
        "ax.add_feature(cfeature.COASTLINE)\n",
        "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
        "ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
        "ax.add_feature(cfeature.RIVERS)\n",
        "\n",
        "# Set map extent to focus on China\n",
        "ax.set_extent([73, 135, 18, 54])  # Adjust as needed\n",
        "\n",
        "# Scatter plot for data points\n",
        "scatter = ax.scatter(df['lon'], df['lat'], c=df['avg'],\n",
        "                    cmap='viridis', s=20, alpha=0.7,\n",
        "                    transform=ccrs.PlateCarree())  # Important: specify transform\n",
        "\n",
        "# Add colorbar\n",
        "cbar = fig.colorbar(scatter, ax=ax, label='PFAS (avg)')\n",
        "\n",
        "# Set map title and labels\n",
        "ax.set_title('PFAS Data Collection Map in China')\n",
        "ax.set_xlabel('Longitude')\n",
        "ax.set_ylabel('Latitude')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WfId_ReuYQmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Counts\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "poid_counts = df['poid'].value_counts()\n",
        "\n",
        "# Create the bar plot\n",
        "poid_counts.plot(kind='bar', figsize=(10, 6))\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Counts of Each POID')\n",
        "plt.xlabel('POID')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
        "plt.tight_layout()  # Adjust layout to prevent overlapping elements\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yW_X-aQ1Yons"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "df_augmentedcombined = pd.read_csv('augmentedcombined.csv')\n",
        "\n",
        "# Drop rows where the target variable 'avg' is NaN\n",
        "df_augmentedcombined.dropna(subset=['avg'], inplace=True)\n",
        "\n",
        "# Clean the 'avg' column: replace '<0.1' with 0 and convert to numeric\n",
        "df_augmentedcombined['avg'] = df_augmentedcombined['avg'].replace('<0.1', '0').astype(float)\n",
        "\n",
        "# Define target variable\n",
        "y = df_augmentedcombined['avg']\n",
        "\n",
        "# Define features (exclude 'id' and 'avg')\n",
        "X = df_augmentedcombined.drop(['id', 'avg'], axis=1)\n",
        "\n",
        "# Identify categorical columns for one-hot encoding\n",
        "# Assuming 'organ' and 'poid' are categorical\n",
        "categorical_features = ['organ', 'poid']\n",
        "# Identify numerical columns\n",
        "numerical_features = X.columns.drop(categorical_features).tolist()\n",
        "\n",
        "# Create transformers for preprocessing and imputation\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', SimpleImputer(strategy='mean'), numerical_features), # Impute numerical with mean\n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # Impute categorical with a constant\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore')) # One-hot encode categorical\n",
        "        ]), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline with preprocessing and DecisionTreeRegressor\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('regressor', DecisionTreeRegressor(random_state=42))])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_test) # This should be r2_score(y_test, y_pred) - Corrected in the next step.\n",
        "\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'Root Mean Squared Error: {rmse}')\n",
        "print(f'R-squared: {r2}')"
      ],
      "metadata": {
        "id": "aOo8ndTjZB0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    df = pd.read_csv(\"augmentedcombined.csv\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV: {e}\")\n",
        "    raise\n",
        "\n",
        "# Define the target variable and columns to exclude\n",
        "target_variable = 'avg'\n",
        "columns_to_exclude = ['province_c', 'locality_c', 'longitude', 'latitude', target_variable]\n",
        "\n",
        "# Identify features\n",
        "features = [col for col in df.columns if col not in columns_to_exclude]\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df[features]\n",
        "y = df[target_variable]\n",
        "\n",
        "# Preprocessing\n",
        "# 1. Handle missing values\n",
        "# For simplicity, we'll fill missing numerical values with the mean and categorical with the mode.\n",
        "# A more sophisticated approach might be needed depending on the dataset's characteristics.\n",
        "for col in X.columns:\n",
        "    if X[col].isnull().any():\n",
        "        if pd.api.types.is_numeric_dtype(X[col]):\n",
        "            X[col].fillna(X[col].mean(), inplace=True)\n",
        "        else: # Assuming categorical\n",
        "            X[col].fillna(X[col].mode()[0], inplace=True) # Take the first mode if multiple\n",
        "\n",
        "if y.isnull().any():\n",
        "    print(f\"Warning: Target variable '{target_variable}' contains {y.isnull().sum()} missing values. These rows will be dropped.\")\n",
        "    # Drop rows where target is NaN\n",
        "    df_cleaned = df.dropna(subset=[target_variable])\n",
        "    X = df_cleaned[features]\n",
        "    y = df_cleaned[target_variable]\n",
        "    # Re-apply missing value imputation for X if its dimensions changed\n",
        "    for col in X.columns:\n",
        "        if X[col].isnull().any():\n",
        "            if pd.api.types.is_numeric_dtype(X[col]):\n",
        "                X[col].fillna(X[col].mean(), inplace=True)\n",
        "            else:\n",
        "                X[col].fillna(X[col].mode()[0], inplace=True)\n",
        "\n",
        "\n",
        "# 2. Encode categorical features\n",
        "# Identify categorical columns that need encoding\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "label_encoders = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    # Fit on all data (X[col]) to ensure all categories are known, then transform\n",
        "    # Handle potential new categories in test set during a real-world scenario if splitting before encoding\n",
        "    X[col] = le.fit_transform(X[col].astype(str)) # Convert to string to handle mixed types if any\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Split the data\n",
        "try:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "except ValueError as e:\n",
        "    print(f\"Error during data splitting: {e}\")\n",
        "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
        "    if X.shape[0] == 0 or y.shape[0] == 0:\n",
        "        print(\"No data available for training after preprocessing. This might be due to all target values being NaN initially.\")\n",
        "    raise\n",
        "\n",
        "# Train the Random Forest Regressor model\n",
        "rf_regressor = RandomForestRegressor(random_state=42)\n",
        "try:\n",
        "    rf_regressor.fit(X_train, y_train)\n",
        "except Exception as e:\n",
        "    print(f\"Error during model training: {e}\")\n",
        "    raise\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print model performance\n",
        "print(f\"Random Forest Regressor Model Performance for target 'avg':\")\n",
        "print(f\"Features used: {features}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"R-squared (R2 ): {r2:.4f}\")\n",
        "\n",
        "# Feature importances (optional, but informative)\n",
        "importances = rf_regressor.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Check if any data was left for training.\n",
        "if X.shape[0] == 0:\n",
        "    final_message = \"The Random Forest Regression could not be performed as there was no data left after handling missing values in the target variable 'avg'.\"\n",
        "else:\n",
        "    final_message = (\n",
        "        \"A Random Forest Regression model has been successfully trained on the dataset \"\n",
        "        f\"to predict the target variable 'avg', excluding 'province_c', 'locality_c', 'longitude', and 'latitude'.\\n\\n\"\n",
        "        f\"Model Performance:\\n\"\n",
        "        f\"- Mean Squared Error (MSE): {mse:.4f}\\n\"\n",
        "        f\"- Root Mean Squared Error (RMSE): {rmse:.4f}\\n\"\n",
        "        f\"- R-squared (R2): {r2:.4f}\\n\\n\"\n",
        "        \"The features used for training were: \" + \", \".join(features) + \".\\n\"\n",
        "        \"Feature importances have also been calculated and printed in the logs.\"\n",
        "    )\n",
        "\n",
        "print(f\"\\nFinal User Message: {final_message}\")"
      ],
      "metadata": {
        "id": "Mb0EOhBUZjtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fixed Random Forest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "df_augmentedcombined = pd.read_csv('augmentedcombined.csv')\n",
        "\n",
        "# Drop rows where the target variable 'avg' is NaN before outlier detection\n",
        "df_augmentedcombined.dropna(subset=['avg'], inplace=True)\n",
        "\n",
        "# Clean the 'avg' column: replace '<0.1' with 0 and convert to numeric\n",
        "df_augmentedcombined['avg'] = df_augmentedcombined['avg'].replace('<0.1', '0').astype(float)\n",
        "\n",
        "# --- Outlier Removal ---\n",
        "# Calculate Q1, Q3, and IQR for the 'avg' column\n",
        "Q1 = df_augmentedcombined['avg'].quantile(0.25)\n",
        "Q3 = df_augmentedcombined['avg'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define bounds for outliers\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Remove outliers\n",
        "df_cleaned = df_augmentedcombined[(df_augmentedcombined['avg'] >= lower_bound) & (df_augmentedcombined['avg'] <= upper_bound)].copy()\n",
        "# --- End Outlier Removal ---\n",
        "\n",
        "# Define target variable from the cleaned data\n",
        "y = df_cleaned['avg']\n",
        "\n",
        "# Define features (exclude 'id' and 'avg') from the cleaned data\n",
        "X = df_cleaned.drop(['id', 'avg'], axis=1)\n",
        "\n",
        "# Identify categorical columns for one-hot encoding\n",
        "# Assuming 'organ' and 'poid' are categorical\n",
        "categorical_features = ['organ', 'poid']\n",
        "# Identify numerical columns\n",
        "numerical_features = X.columns.drop(categorical_features).tolist()\n",
        "\n",
        "# Create transformers for preprocessing and imputation\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', SimpleImputer(strategy='mean'), numerical_features), # Impute numerical with mean\n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # Impute categorical with a constant\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore')) # One-hot encode categorical\n",
        "        ]), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline with preprocessing and RandomForestRegressor\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('regressor', RandomForestRegressor(random_state=42))])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Mean Squared Error (after outlier removal): {mse}')\n",
        "print(f'Root Mean Squared Error (after outlier removal): {rmse}')\n",
        "print(f'R-squared (after outlier removal): {r2}')"
      ],
      "metadata": {
        "id": "dOJYuvdjZ-NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XG Boost\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('augmentedcombined.csv')\n",
        "\n",
        "## STEP 1: Data Cleaning and Type Conversion\n",
        "def convert_to_numeric(df, columns):\n",
        "    for col in columns:\n",
        "        if col in df.columns:\n",
        "            # Convert to string first to handle mixed types, then to numeric\n",
        "            df[col] = pd.to_numeric(df[col].astype(str), errors='coerce')\n",
        "    return df\n",
        "\n",
        "# Convert numeric columns\n",
        "numeric_cols = ['avg', 'max', 'min', 'IndustryValue', 'EnvEmerg', 'NetIndustryValue',\n",
        "                'PollutantEmission', 'WasteGas', 'RealEstateDev', 'WasteDisposalVol',\n",
        "                'NumWasteFactories', 'MajorEnergyConsumption', 'IndustryValue_Overall',\n",
        "                'IndustrialEnterprise']\n",
        "\n",
        "data = convert_to_numeric(data, numeric_cols)\n",
        "\n",
        "## STEP 2: IQR Outlier Removal Function (with NaN handling)\n",
        "def remove_outliers_iqr(df, column):\n",
        "    if column not in df.columns:\n",
        "        return df\n",
        "\n",
        "    # Calculate quartiles only for non-null values\n",
        "    col_data = df[column].dropna()\n",
        "    if len(col_data) == 0:\n",
        "        return df\n",
        "\n",
        "    Q1 = col_data.quantile(0.25)\n",
        "    Q3 = col_data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound) | (df[column].isna())]\n",
        "\n",
        "## STEP 3: Apply Outlier Removal\n",
        "clean_data = data.copy()\n",
        "\n",
        "# First remove outliers from target variable\n",
        "clean_data = remove_outliers_iqr(clean_data, 'avg')\n",
        "\n",
        "# Then remove outliers from numeric features (optional)\n",
        "for col in numeric_cols:\n",
        "    clean_data = remove_outliers_iqr(clean_data, col)\n",
        "\n",
        "## STEP 4: Prepare Data for Modeling\n",
        "X = clean_data.drop(columns=['avg', 'id'])\n",
        "y = clean_data['avg']\n",
        "\n",
        "# Handle categorical columns safely\n",
        "categorical_cols = ['poid', 'organ']\n",
        "for col in categorical_cols:\n",
        "    if col in X.columns:\n",
        "        # Convert to category codes, handling NaN values\n",
        "        X[col] = X[col].astype('category').cat.codes\n",
        "\n",
        "## STEP 5: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "## STEP 6: XGBoost Model with Missing Value Handling\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)\n",
        "\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'tree_method': 'hist',\n",
        "    'eta': 0.05,\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'eval_metric': 'rmse'\n",
        "}\n",
        "\n",
        "model = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=1000,\n",
        "    evals=[(dtrain, 'train'), (dtest, 'test')],\n",
        "    early_stopping_rounds=10,\n",
        "    verbose_eval=50\n",
        ")\n",
        "\n",
        "## STEP 7: Evaluation\n",
        "y_pred = model.predict(dtest)\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(f\"RMSE: {mean_squared_error(y_test, y_pred, squared=False):.2f}\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "print(f\"RÂ²: {r2_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "id": "29-anf_TYvJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the trained regressor model from the pipeline\n",
        "regressor = model.named_steps['regressor']\n",
        "\n",
        "# Get feature names after preprocessing\n",
        "preprocessor = model.named_steps['preprocessor']\n",
        "feature_names_out = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = regressor.feature_importances_\n",
        "\n",
        "# Create a pandas Series for better visualization\n",
        "feature_importances_series = pd.Series(feature_importances, index=feature_names_out)\n",
        "\n",
        "# Sort feature importances\n",
        "sorted_feature_importances = feature_importances_series.sort_values(ascending=False)\n",
        "\n",
        "# Print sorted feature importances\n",
        "print(\"Feature Importances (after outlier removal):\")\n",
        "print(sorted_feature_importances.to_markdown(numalign=\"left\", stralign=\"left\"))"
      ],
      "metadata": {
        "id": "l7uDcsotZtoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GNB\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df_augmentedcombined = pd.read_csv('augmentedcombined.csv')\n",
        "\n",
        "# Drop rows where the target variable 'avg' is NaN\n",
        "df_augmentedcombined.dropna(subset=['avg'], inplace=True)\n",
        "\n",
        "# Clean the 'avg' column: replace '<0.1' with 0 and convert to numeric\n",
        "df_augmentedcombined['avg'] = df_augmentedcombined['avg'].replace('<0.1', '0').astype(float)\n",
        "\n",
        "# Categorize 'avg' into 'low', 'medium', 'high'\n",
        "# Using quantiles to define bins\n",
        "q_low = df_augmentedcombined['avg'].quantile(0.33)\n",
        "q_high = df_augmentedcombined['avg'].quantile(0.66)\n",
        "\n",
        "def categorize_avg(avg_value):\n",
        "    if avg_value <= q_low:\n",
        "        return 'low'\n",
        "    elif avg_value <= q_high:\n",
        "        return 'medium'\n",
        "    else:\n",
        "        return 'high'\n",
        "\n",
        "df_augmentedcombined['avg_category'] = df_augmentedcombined['avg'].apply(categorize_avg)\n",
        "\n",
        "# Define target variable (categorical 'avg')\n",
        "y = df_augmentedcombined['avg_category']\n",
        "\n",
        "# Define features (exclude 'id', original 'avg', and the new 'avg_category')\n",
        "X = df_augmentedcombined.drop(['id', 'avg', 'avg_category'], axis=1)\n",
        "\n",
        "# Identify categorical columns for one-hot encoding\n",
        "# Assuming 'organ' and 'poid' are categorical\n",
        "categorical_features = ['organ', 'poid']\n",
        "# Identify numerical columns\n",
        "numerical_features = X.columns.drop(categorical_features).tolist()\n",
        "\n",
        "# Create transformers for preprocessing and imputation\n",
        "# Gaussian Naive Bayes is sensitive to feature scaling, but for one-hot encoded features and\n",
        "# imputation as done here, scaling is not strictly necessary within this preprocessor\n",
        "# for GNB itself, though it could be beneficial for other models.\n",
        "# However, to maintain consistency with previous steps and handle NaNs and categories,\n",
        "# we keep the same preprocessing structure.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', SimpleImputer(strategy='mean'), numerical_features), # Impute numerical with mean\n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # Impute categorical with a constant\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore')) # One-hot encode categorical\n",
        "        ]), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline with preprocessing and GaussianNB classifier\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('classifier', GaussianNB())])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print('Classification Report:')\n",
        "print(report)"
      ],
      "metadata": {
        "id": "lk3EoI1DaMXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SVM\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df_augmentedcombined = pd.read_csv('augmentedcombined.csv')\n",
        "\n",
        "# Drop rows where the target variable 'avg' is NaN\n",
        "df_augmentedcombined.dropna(subset=['avg'], inplace=True)\n",
        "\n",
        "# Clean the 'avg' column: replace '<0.1' with 0 and convert to numeric\n",
        "df_augmentedcombined['avg'] = df_augmentedcombined['avg'].replace('<0.1', '0').astype(float)\n",
        "\n",
        "# Categorize 'avg' into 'low', 'medium', 'high'\n",
        "# Using quantiles to define bins\n",
        "q_low = df_augmentedcombined['avg'].quantile(0.33)\n",
        "q_high = df_augmentedcombined['avg'].quantile(0.66)\n",
        "\n",
        "def categorize_avg(avg_value):\n",
        "    if avg_value <= q_low:\n",
        "        return 'low'\n",
        "    elif avg_value <= q_high:\n",
        "        return 'medium'\n",
        "    else:\n",
        "        return 'high'\n",
        "\n",
        "df_augmentedcombined['avg_category'] = df_augmentedcombined['avg'].apply(categorize_avg)\n",
        "\n",
        "# Define target variable (categorical 'avg')\n",
        "y = df_augmentedcombined['avg_category']\n",
        "\n",
        "# Define features (exclude 'id', original 'avg', and the new 'avg_category')\n",
        "X = df_augmentedcombined.drop(['id', 'avg', 'avg_category'], axis=1)\n",
        "\n",
        "# Identify categorical columns for one-hot encoding\n",
        "# Assuming 'organ' and 'poid' are categorical\n",
        "categorical_features = ['organ', 'poid']\n",
        "# Identify numerical columns\n",
        "numerical_features = X.columns.drop(categorical_features).tolist()\n",
        "\n",
        "# Create transformers for preprocessing and imputation\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', SimpleImputer(strategy='mean'), numerical_features), # Impute numerical with mean\n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # Impute categorical with a constant\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore')) # One-hot encode categorical\n",
        "        ]), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline with preprocessing and SVC classifier\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('classifier', SVC(random_state=42))]) # Using default SVC parameters\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print('Classification Report:')\n",
        "print(report)"
      ],
      "metadata": {
        "id": "urTDUQqFaO6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df_augmentedcombined = pd.read_csv('augmentedcombined.csv')\n",
        "\n",
        "# Drop rows where the target variable 'avg' is NaN\n",
        "df_augmentedcombined.dropna(subset=['avg'], inplace=True)\n",
        "\n",
        "# Clean the 'avg' column: replace '<0.1' with 0 and convert to numeric\n",
        "df_augmentedcombined['avg'] = df_augmentedcombined['avg'].replace('<0.1', '0').astype(float)\n",
        "\n",
        "# Define bins for categorization based on quantiles\n",
        "# Use quantiles to create roughly equal-sized categories\n",
        "low_threshold = df_augmentedcombined['avg'].quantile(0.33)\n",
        "high_threshold = df_augmentedcombined['avg'].quantile(0.66)\n",
        "\n",
        "# Create categorical 'avg' column\n",
        "def categorize_avg(avg_value):\n",
        "    if avg_value <= low_threshold:\n",
        "        return 'low'\n",
        "    elif avg_value <= high_threshold:\n",
        "        return 'medium'\n",
        "    else:\n",
        "        return 'high'\n",
        "\n",
        "df_augmentedcombined['avg_category'] = df_augmentedcombined['avg'].apply(categorize_avg)\n",
        "\n",
        "\n",
        "# Define target variable\n",
        "y = df_augmentedcombined['avg_category']\n",
        "\n",
        "# Define features (exclude 'id', original 'avg', and the new 'avg_category')\n",
        "X = df_augmentedcombined.drop(['id', 'avg', 'avg_category'], axis=1)\n",
        "\n",
        "# Identify categorical columns for one-hot encoding\n",
        "categorical_features = ['organ', 'poid']\n",
        "# Identify numerical columns\n",
        "numerical_features = X.columns.drop(categorical_features).tolist()\n",
        "\n",
        "# Create transformers for preprocessing and imputation\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', SimpleImputer(strategy='mean'), numerical_features), # Impute numerical with mean\n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # Impute categorical with a constant\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore')) # One-hot encode categorical\n",
        "        ]), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline with preprocessing and LogisticRegression\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('classifier', LogisticRegression(random_state=42, multi_class='auto', solver='liblinear'))]) # Using liblinear solver suitable for smaller datasets\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Classification Report:\\n{class_report}')"
      ],
      "metadata": {
        "id": "VUHdzNh9aeBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset\n",
        "df_augmentedcombined = pd.read_csv('augmentedcombined.csv')\n",
        "\n",
        "# Define the target variable and features\n",
        "y = df_augmentedcombined['avg']\n",
        "X = df_augmentedcombined.drop(['id', 'avg'], axis=1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest Regressor model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")"
      ],
      "metadata": {
        "id": "NzdfHCkkaji1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}